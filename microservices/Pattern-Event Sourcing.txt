									Event Sourcing and Domain Events
									      Event Driven Microservices
							Messaging - use asynchronous messaging for inter-service communication


An Event-microservices architecture is an approach to software development where decoupled microservices are designed to communicate with one another when events occur.

Event sourcing
Domain event - Inspired From Domain Driven Design
 
  Both are same , which are different from only based model we select.
if you select DDD, you can follow designing "events" using domain events.


1.Context
2.Problem
3.Forces
4.Solution
5.Resulting Context
6.Related Patterns

1.Context
  A service "command" typically needs to create/update/delete aggregates in the database and send messages/events to a message broker.

Note: 
command-verb-method
aggregates - A graph of objects that can be treated as a unit. (From DDD)

 "Event Sourcing is an alternative way to persist data". In contrast with "state-oriented" persistence that only keeps the latest version of the entity state, Event sourcing stores each state mutation as separate record called event.

When user starts interaction , when making order...
   
 Order :                 Order :                Order
  Number : 1220            Number : 1220         Number : 1220
  status : STARTED  ---->  status :PENDING ----> status: CONFIRMED | REJECTED
  total : 200               total : 200          total : 200
  paid : 0                  paid: 0              paid : 5000
   
  
UPDATE QUERY - status=STARTED
UPDATE QUERY - status=PENDING
UPDATE QUERY - status=CONFIRMED

History of Transaction

started    pending     confirmed
 |
------------------------------------------------------------------
 |          |              |
   
log      log              log  ------>EVENT store -can be any db or brokers
			  

Problem
 How to atomically update the database and send messages to a message broker?


Solution
   A good solution to this problem is to use event sourcing. Event sourcing persists the state of a business entity such an Order or a Customer as a sequence of state-changing events.

Resulting context

1.It solves one of the key problems in implementing an event-driven architecture and makes it possible to reliably publish events whenever state changes.

2.Because it persists events rather than domain objects, it mostly avoids the object‑relational impedance mismatch problem.

3.It provides a 100% reliable audit log of the changes made to a business entity
 It makes it possible to implement temporal queries that determine the state of an  entity at any point in time.

4.Event sourcing-based business logic consists of loosely coupled business entities that exchange events. This makes it a lot easier to migrate from a monolithic application to a microservice architecture

Related patterns
..................
1.The Saga and Domain event patterns create the need for this pattern.
2.The CQRS must often be used with event sourcing.
3.Event sourcing implements the Audit logging pattern.



Eventsourcing with "eventStore as Database table"
.................................................


Implementation:

Use Case:

Mr Subramanian has a shop
He is sells electronic items like mobile phones, laptops etc
He wants to keep track of the stock in his shop.


App functionality:

1.Add new stock
2.Remove existing stock
3.find current stock of particular item.

Initially this app built using traditional way : without event sourcing pattern.

There is a table stock table , when ever new product added stock is added or when ever product is removed(sold), stock is updated.

when ever stock is added or removed current state updated.

This same operation is done by another co worker of Subramanian who is Mr. Ram.

One day Subramanian got doubt something went wrong in the stock, now he realized existing system cant be tracked what happened.
when ever new stock is added or removed existing one, we cant track it.

He found a solution to solve this issue by "Event Sourcing Pattern"

You can capture user events and add them in "Event Store"

Molding Events:
"StockAddedEvent"
"StockRemovedEvent"
 
You can store these events in relational database or event platforms like Kafka.


Why Use Event Sourcing?
| Feature                | Benefit                                        |
| ---------------------- | ---------------------------------------------- |
| **Audit Trail**        | Complete history of what happened              |
| **Temporal Queries**   | Rebuild state at any point in time             |
| **Decoupled Services** | Events can be published to other microservices |
| **Reliable State**     | Prevents accidental data overwrite             |

Flow:

Client
  ↓
[API Layer - Controller]
  ↓
[Command Layer - Service]
  ↓
[Event Store (DB)]
  ↓                    ↘
Event Publisher     [Event Handlers] → Update Projections (Read DB)

Key Concepts
..........................
Event: Immutable object that represents a state change (e.g., OrderCreatedEvent, OrderShippedEvent).
Aggregate: Domain object built by applying events.
Event Store: DB table where events are appended.
Projector: Listens to events and updates read models.

A hybrid Event Sourcing + traditional persistence approach, where:

The current state (like Product, Order) is stored in a regular table (like products, orders)
The events (OrderCreatedEvent, ProductAddedEvent, etc.) are stored in an event store table (like event_store)

Use Case:
1.When a product is inserted:
2.Save product in products table
3.Emit OrderCreatedEvent (or similar event)
4.Save event in event_store table
&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&


ex:
pom.xml
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
	xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd">
	<modelVersion>4.0.0</modelVersion>
	<parent>
		<groupId>org.springframework.boot</groupId>
		<artifactId>spring-boot-starter-parent</artifactId>
		<version>3.3.1</version>
		<relativePath/> <!-- lookup parent from repository -->
	</parent>
	<groupId>com.ibm</groupId>
	<artifactId>eventsourcingusingdb</artifactId>
	<version>0.0.1-SNAPSHOT</version>
	<name>eventsourcingusingdb</name>
	<description>Demo project for Spring Boot</description>
	<url/>
	<licenses>
		<license/>
	</licenses>
	<developers>
		<developer/>
	</developers>
	<scm>
		<connection/>
		<developerConnection/>
		<tag/>
		<url/>
	</scm>
	<properties>
		<java.version>17</java.version>
	</properties>
	<dependencies>
		<dependency>
			<groupId>org.springframework.boot</groupId>
			<artifactId>spring-boot-starter-data-jpa</artifactId>
		</dependency>
		<dependency>
			<groupId>org.springframework.boot</groupId>
			<artifactId>spring-boot-starter-web</artifactId>
		</dependency>
		<!-- https://mvnrepository.com/artifact/com.google.code.gson/gson -->
		<dependency>
			<groupId>com.google.code.gson</groupId>
			<artifactId>gson</artifactId>
			<version>2.11.0</version>
		</dependency>

		<dependency>
			<groupId>com.h2database</groupId>
			<artifactId>h2</artifactId>
			<scope>runtime</scope>
		</dependency>
		<dependency>
			<groupId>org.projectlombok</groupId>
			<artifactId>lombok</artifactId>
			<optional>true</optional>
		</dependency>
		<dependency>
			<groupId>org.springframework.boot</groupId>
			<artifactId>spring-boot-starter-test</artifactId>
			<scope>test</scope>
		</dependency>
	</dependencies>

	<build>
		<plugins>
			<plugin>
				<groupId>org.springframework.boot</groupId>
				<artifactId>spring-boot-maven-plugin</artifactId>
				<configuration>
					<excludes>
						<exclude>
							<groupId>org.projectlombok</groupId>
							<artifactId>lombok</artifactId>
						</exclude>
					</excludes>
				</configuration>
			</plugin>
		</plugins>
	</build>

</project>

application.properties
spring.application.name=eventsourcingusingdb
spring.datasource.url=jdbc:h2:mem:testdb
spring.datasource.driverClassName=org.h2.Driver
spring.datasource.username=sa
spring.datasource.password=
spring.datasource.jpa.database-platform=org.hibernate.dialect.H2Dialect
spring.h2.console.enabled=true
spring.h2.console.path=/h2

Stock.java
package com.sunlife.eventsourcing;

import lombok.Data;

@Data
public class Stock {
    private String name;
    private int quantity;
    private String user;
}

Event: Record
package com.sunlife.eventsourcing;

public interface StockEvent {
}

package com.sunlife.eventsourcing;

import lombok.Builder;
import lombok.Data;

@Data
@Builder
public class StockAddedEvent implements StockEvent {
    private  Stock stockDetails;
}

package com.sunlife.eventsourcing;

import lombok.Builder;
import lombok.Data;

@Builder
@Data
public class StockRemovedEvent implements StockEvent {
    private Stock stockDetails;
}
.....................
Repository:
 -Store Stock Information
 -Stock Event information

................
package com.ibm.eventsourcing.db;


import jakarta.persistence.*;
import lombok.Data;

@Data
@Entity
@Table(name = "stock")
public class Stock {
    @Id
    @GeneratedValue
    public Long id;
    @Column(name = "name")
    public String name;
    @Column(name = "qty")
    public int quantity;
    @Column(name = "userName")
    public String user;
}

package com.ibm.eventsourcing.db;

import jakarta.persistence.Entity;
import jakarta.persistence.GeneratedValue;
import jakarta.persistence.GenerationType;
import jakarta.persistence.Id;
import lombok.Data;

import java.time.LocalDateTime;

@Data
@Entity
public class EventStore {
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private long eventId;
    private String eventType;
    private String entityId;
    private String eventData;
    private LocalDateTime eventTime;
}

package com.ibm.eventsourcing.db;

import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.data.repository.CrudRepository;
import org.springframework.stereotype.Component;
import org.springframework.stereotype.Repository;

import java.time.LocalDateTime;

@Repository
public interface EventRepository extends JpaRepository<EventStore, Long> {
    Iterable<EventStore> findByEntityId(String entityId);
    Iterable<EventStore> findByEntityIdAndEventTimeLessThanEqual(String entityId, LocalDateTime date);
}
package com.ibm.eventsourcing.db;

import org.springframework.data.repository.CrudRepository;
import org.springframework.stereotype.Repository;
import java.util.List;

@Repository
public interface StockRepository extends CrudRepository<Stock, Long> {
    List<Stock> findByName(String name);
}
............
Events
...........
package com.ibm.eventsourcing.db;

public interface StockEvent {
}

package com.ibm.eventsourcing.db;

import lombok.Builder;
import lombok.Data;

@Data
@Builder
public class StockAddedEvent implements StockEvent {
    private Stock stockDetails;
}
package com.ibm.eventsourcing.db;

import lombok.Builder;
import lombok.Data;

@Data
@Builder
public class StockUpdatedEvent implements StockEvent{
    private Stock stockDetails;
}
package com.ibm.eventsourcing.db;

import lombok.Builder;
import lombok.Data;

@Builder
@Data
public class StockRemovedEvent implements StockEvent {
    private Stock stockDetails;
}
..
service:
package com.ibm.eventsourcing.db;

import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.databind.ObjectMapper;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Service;

import java.time.LocalDateTime;

@Service
public class EventService {
    @Autowired
    private EventRepository repository;

    public void addEvent(StockAddedEvent event) throws JsonProcessingException {
        EventStore eventStore = new EventStore();
        eventStore.setEventData(new ObjectMapper().writeValueAsString(event.getStockDetails()));
        eventStore.setEventType("STOCK_ADDED");
        eventStore.setEntityId(event.getStockDetails().getName());
        eventStore.setEventTime(LocalDateTime.now());
        repository.save(eventStore);
    }

    public void addEvent(StockRemovedEvent event) throws JsonProcessingException {
        EventStore eventStore = new EventStore();
        eventStore.setEventData(new ObjectMapper().writeValueAsString(event.getStockDetails()));
        eventStore.setEventType("STOCK_REMOVED");
        eventStore.setEntityId(event.getStockDetails().getName());
        eventStore.setEventTime(LocalDateTime.now());
        repository.save(eventStore);
    }
    public void addEvent(StockUpdatedEvent event) throws JsonProcessingException {
        EventStore eventStore = new EventStore();
        eventStore.setEventData(new ObjectMapper().writeValueAsString(event.getStockDetails()));
        eventStore.setEventType("STOCK_UPDATED");
        eventStore.setEntityId(event.getStockDetails().getName());
        eventStore.setEventTime(LocalDateTime.now());
        repository.save(eventStore);
    }

    public Iterable<EventStore> fetchAllEvents(String name) {
        return repository.findByEntityId(name);
    }

    public Iterable<EventStore> fetchAllEventsTillDate(String name, LocalDateTime date) {
        return repository.findByEntityIdAndEventTimeLessThanEqual(name, date);

    }
}
package com.ibm.eventsourcing.db;

import com.fasterxml.jackson.core.JsonProcessingException;
import com.google.gson.Gson;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;

import java.time.LocalDate;
import java.time.LocalDateTime;
import java.util.List;

@RestController
public class StockController {
    @Autowired
    private EventService eventService;
    @Autowired
    private StockRepository stockRepository;

    @PostMapping("/stock")
    public ResponseEntity<Stock> addStock(@RequestBody Stock stockRequest) throws JsonProcessingException {
        StockAddedEvent addedEvent = StockAddedEvent.builder().stockDetails(stockRequest).build();
        StockUpdatedEvent updatedEvent = StockUpdatedEvent.builder().stockDetails(stockRequest).build();
        List<Stock> existingStockList = stockRepository.findByName(stockRequest.getName());
        if (existingStockList != null && existingStockList.size() > 0) {
            Stock existingStock = existingStockList.get(0);
            int newQuantity = existingStock.getQuantity() + stockRequest.getQuantity();
            //update logic
            //existingStock.quantity = newQuantity;
            existingStock.setQuantity(newQuantity);
            //existingStock.name = stockRequest.name;
            existingStock.setName(stockRequest.getName());
            updatedEvent.setStockDetails(existingStock);
            //fire update event
            eventService.addEvent(updatedEvent);
            return ResponseEntity.ok(existingStock); // 200 OK
        } else {
            stockRepository.save(stockRequest); // This was a bug: previously `s(...)`
            eventService.addEvent(addedEvent);
            return ResponseEntity.status(201).body(stockRequest); // 201 Created
        }
    }

    @DeleteMapping("/stock")
    public void removeStock(@RequestBody Stock stock) throws JsonProcessingException {
        StockRemovedEvent event = StockRemovedEvent.builder().stockDetails(stock).build();
        int newQuantity = 0;
        List<Stock> existingStockList = stockRepository.findByName(stock.getName());

        if (existingStockList != null && existingStockList.size() > 0) {

            Stock existingStock = existingStockList.get(0);

            newQuantity = existingStock.getQuantity() - stock.getQuantity();

            if (newQuantity <= 0) {
                stockRepository.delete(existingStock);
            } else {
                existingStock.setQuantity(newQuantity);
                  existingStock.setUser(stock.getUser());
                stockRepository.save(existingStock);
            }
            event.setStockDetails(existingStock);
        }

        eventService.addEvent(event);
    }

    @GetMapping("/stock")
    public Stock getStock(@RequestParam("name") String name) throws JsonProcessingException {
        Iterable<EventStore> events = eventService.fetchAllEvents(name);
        Stock currentStock = new Stock();
        currentStock.setName(name);
        currentStock.setUser("NA");
        for (EventStore event : events) {
            Stock stock = new Gson().fromJson(event.getEventData(), Stock.class);
            if (event.getEventType().equals("STOCK_ADDED")) {
                currentStock.setQuantity(currentStock.getQuantity() + stock.getQuantity());
            } else if (event.getEventType().equals("STOCK_REMOVED")) {
                currentStock.setQuantity(currentStock.getQuantity() - stock.getQuantity());
            }
        }
        return currentStock;
    }

    @GetMapping("/events")
    public Iterable<EventStore> getEvents(@RequestParam("name") String name) throws JsonProcessingException {
        Iterable<EventStore> events = eventService.fetchAllEvents(name);
        return events;
    }

    //History of events.
    @GetMapping("/stock/history")
    public Stock getStockUntilDate(@RequestParam("date") String date, @RequestParam("name") String name) throws JsonProcessingException {

        String[] dateArray = date.split("-");

        LocalDateTime dateTill = LocalDate.of(Integer.parseInt(dateArray[0]), Integer.parseInt(dateArray[1]), Integer.parseInt(dateArray[2])).atTime(23, 59);


        Iterable<EventStore> events = eventService.fetchAllEventsTillDate(name, dateTill);

        Stock currentStock = new Stock();

        currentStock.setName(name);
        currentStock.setUser("NA");

        for (EventStore event : events) {

            Stock stock = new Gson().fromJson(event.getEventData(), Stock.class);

            if (event.getEventType().equals("STOCK_ADDED")) {

                currentStock.setQuantity(currentStock.getQuantity() + stock.getQuantity());
            } else if (event.getEventType().equals("STOCK_REMOVED")) {

                currentStock.setQuantity(currentStock.getQuantity() - stock.getQuantity());
            }
        }

        return currentStock;

    }


}
package com.ibm;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;

@SpringBootApplication
public class EventSourcingUsingDatabase {

	public static void main(String[] args) {
		SpringApplication.run(EventSourcingUsingDatabase.class, args);
	}

}

How to test;

POST localhost:8080/stock

{
    "name": "IPhone",
    "quantity": 10,
    "user": "Ram"
}

DELETE  localhost:8080/stock
{
    "name": "IPhone",
    "quantity": 10,
    "user": "Ram"
}

GET localhost:8080/events?name=IPhone
[
    {
        "eventId": 1,
        "eventType": "STOCK_ADDED",
        "entityId": "IPhone",
        "eventData": "{\"id\":1,\"name\":\"IPhone\",\"quantity\":10,\"user\":\"Ram\"}",
        "eventTime": "2025-08-06T11:25:01.470341"
    },
    {
        "eventId": 2,
        "eventType": "STOCK_UPDATED",
        "entityId": "IPhone",
        "eventData": "{\"id\":1,\"name\":\"IPhone\",\"quantity\":20,\"user\":\"Ram\"}",
        "eventTime": "2025-08-06T11:25:02.550263"
    },
    {
        "eventId": 3,
        "eventType": "STOCK_UPDATED",
        "entityId": "IPhone",
        "eventData": "{\"id\":1,\"name\":\"IPhone\",\"quantity\":30,\"user\":\"Ram\"}",
        "eventTime": "2025-08-06T11:25:03.710183"
    },
    {
        "eventId": 4,
        "eventType": "STOCK_REMOVED",
        "entityId": "IPhone",
        "eventData": "{\"id\":1,\"name\":\"IPhone\",\"quantity\":20,\"user\":\"IPhone\"}",
        "eventTime": "2025-08-06T11:25:25.392045"
    }
]
&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&
								  EventSourcing with External Events Store platforms									
&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&

1.Kafka
2.eventStoreDb
3.CloudEventStore
4.Eventuate Tram

Kafka:
.....

What is Kafka?
  Apache Kafka is an open-source distributed event streaming platform.

What is Event?
   An Event is any type of action,incident,or change are "happening" or "just happened"
for eg:
  Now i am typing,Now i am teaching - happening
  Just i had coffee,Just i received mail, just i clicked a link, just i searched product - happened.

 "An Event is just remainder or notification of  your happenings or happened"

Events In the Softwares Systems:
................................
Every Software system has concept of "logs"

Log:
  Recording current informations.
 Logs are used in software to record activities of code.

...webserver initalize.... time.....
...webserver assigns port....
...webserver assigns host...

Logs are used to tracking,debuging,fixing errors etc..... 


Imgaine i need  somebody or somthing should record every activity of my life from the early moring when i get up and till bed.

  There is a system to record every events of your life that is called 
			      Kafka

	 Kafka is Event Processing Software , which stores and process events
...................................................................................
.....................................................................................
			Kafka Basic  Architecture
.....................................................................................

How kafka has been implemented?

   "Kafka is a software"
   "Kafka is a file(Commit log file) processing software
   "Kafka is written in java and scala" - Kafka is just java application
   "In order to run Kafka we need JVM"

How event is represented into kafka?

	Event is just a message.
        Every message has its own arch.
        In Kafka the Event/Message is called as "Record".
		Event(Record)

Event====>Record----------Kafka---will store into log file...
.....................................................................................
			 Sending Messages(Events) to Broker
.....................................................................................	
				Topics
....................................................................................

What is Topic?
  There are lot of events, we need to organize them in the system
  Apache Kafka's most fundamental unit of organization is the topic.

 Topic is just like table in the relational database.

  As we discussed already, kafka just stores events in the log files.

  We never write events into log file directly.

  As a developer we caputure events, write them into "topic" , kafka writes into log file from the topic.

  Topic is log of events, logs are easy to undestand

 Topic is just simple data structure with well known semantics, they are append only.

 When ever we write a message, it always goes on the end.

 When you read message, from the logs, by "Seeking offset in the log".

 Logs are fundamental durable things, Traditional Messaging systems have topics and queues which stores messages temporarily to buffer them between source and designation.

 Since topics are logs, which always permenant.

 You can delete directly log files not but not messages, but you purge messages.

 You can store logs as short as to as long as years or even you can retain messages indefintely.

Partition:
..........

 Breaking topic into multiple units called partitions.

Segments:
  Each partitions is broken up into multiple log files...
.....................................................................................
				 Kafka Broker
.....................................................................................

It is node or process which host kafka application, kafka app is java application.

if you run multiple kakfa process(jvms) on single host or mutliple host or inside vm or containers... : cluster.

Cluster means group of kafka process called broker.

Kafka has two software:
.......................

1.Data plane - Where actual records are stored - Brokers
2.Control Plane - which manages cluster- Cluster Manager

Control Plane:
1.Zookeeper - traditional control plan software.
2.KRaft- modern control plan software
.....................................................................................

Kafka Distribution:

1.Apache Kafka - core kafka -open source
2.Confluent Kafka - Confluent is company who is running by kafka creators, who built Enter prise kafka - community,enterprise...
.....................................................................................

How to work with kafka?

1.You kafka broker
2.You need application written in any language - can talk to kafka

Kafka provides cli tools to learn kafka core features, publishing,consuming etc....


How to setup kafka?

1.Desktop
   Linux,windows
2.Docker 
3.Cloud
........................................................................................................................................................................................................................................
										Building Spring with Kafka Application


Steps :

1.You have start Kafka
docker-compose.yaml

version: '3.8'

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    container_name: zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka
    ports:
      - "9092:9092"
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1

Start
>docker-compose -f docker-compose.yaml up

application.yml
spring:
  kafka:
    producer:
      bootstrap-servers: localhost:9092
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.springframework.kafka.support.serializer.JsonSerializer

  datasource:
    url: jdbc:h2:mem:testdb
    driverClassName: org.h2.Driver
    username: sa
    password:

  jpa:
    database-platform: org.hibernate.dialect.H2Dialect

  h2:
    console:
      enabled: true
      path: /h2

pom.xml
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
	xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd">
	<modelVersion>4.0.0</modelVersion>
	<parent>
		<groupId>org.springframework.boot</groupId>
		<artifactId>spring-boot-starter-parent</artifactId>
		<version>3.2.4</version>
		<relativePath/> <!-- lookup parent from repository -->
	</parent>
	<groupId>com.ibm.eventsourcing</groupId>
	<artifactId>eventsourcing-kakfa</artifactId>
	<version>0.0.1-SNAPSHOT</version>
	<name>eventsourcing-kakfa</name>
	<description>Demo project for Spring Boot</description>
	<properties>
		<java.version>17</java.version>
	</properties>
	<dependencies>
		<dependency>
			<groupId>org.springframework.boot</groupId>
			<artifactId>spring-boot-starter-data-jpa</artifactId>
		</dependency>
		<dependency>
			<groupId>org.springframework.boot</groupId>
			<artifactId>spring-boot-starter-web</artifactId>
		</dependency>
		<dependency>
			<groupId>org.springframework.kafka</groupId>
			<artifactId>spring-kafka</artifactId>
		</dependency>

		<dependency>
			<groupId>com.h2database</groupId>
			<artifactId>h2</artifactId>
			<scope>runtime</scope>
		</dependency>
		<dependency>
			<groupId>org.projectlombok</groupId>
			<artifactId>lombok</artifactId>
			<optional>true</optional>
		</dependency>
		<dependency>
			<groupId>org.springframework.boot</groupId>
			<artifactId>spring-boot-starter-test</artifactId>
			<scope>test</scope>
		</dependency>
		<dependency>
			<groupId>com.google.code.gson</groupId>
			<artifactId>gson</artifactId>
			<version>2.10.1</version>
		</dependency>
		<dependency>
			<groupId>org.springframework.kafka</groupId>
			<artifactId>spring-kafka-test</artifactId>
			<scope>test</scope>
		</dependency>
	</dependencies>

	<build>
		<plugins>
			<plugin>
				<groupId>org.springframework.boot</groupId>
				<artifactId>spring-boot-maven-plugin</artifactId>
				<configuration>
					<excludes>
						<exclude>
							<groupId>org.projectlombok</groupId>
							<artifactId>lombok</artifactId>
						</exclude>
					</excludes>
				</configuration>
			</plugin>
		</plugins>
	</build>

</project>

Entity:
package com.ibm.eventsourcing.kafka;

import jakarta.persistence.Entity;
import jakarta.persistence.GeneratedValue;
import jakarta.persistence.GenerationType;
import jakarta.persistence.Id;
import lombok.Data;
@Entity
@Data
public class Stock {
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private long id;
    private String name;
    private int quantity;
    private String userName;

}
Repository:
package com.ibm.eventsourcing.kafka;

import java.util.List;
import org.springframework.data.repository.CrudRepository;

public interface StockRepo extends CrudRepository<Stock, Integer> {

    List<Stock> findByName(String name);
}

Events:
package com.ibm.eventsourcing.kafka;
import lombok.Data;
import java.time.LocalDateTime;

@Data
public class EventRecord {
    private long eventId;
    private String eventType;
    private String entityId;
    private String eventData;
    private LocalDateTime eventTime;
}

package com.ibm.eventsourcing.kafka;

import lombok.Builder;
import lombok.Data;

@Data
@Builder
public class StockAddedEvent implements StockEvent {
    private  Stock stockDetails;
}
package com.ibm.eventsourcing.kafka;

import lombok.Builder;
import lombok.Data;

@Builder
@Data
public class StockRemovedEvent implements StockEvent {
    private Stock stockDetails;
}

Services:
package com.ibm.eventsourcing.kafka;

import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.databind.ObjectMapper;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.kafka.support.SendResult;
import org.springframework.stereotype.Service;

import java.time.LocalDateTime;
import java.util.Random;
import java.util.UUID;
import java.util.concurrent.CompletableFuture;


@Service
public class EventService {
    //used to send/publish message into kafka broker
    @Autowired
    private KafkaTemplate<String, Object> template;

    public void addEvent(StockAddedEvent event) throws JsonProcessingException {
        EventRecord eventRecord = new EventRecord();
        eventRecord.setEventData(new ObjectMapper().writeValueAsString(event.getStockDetails()));
        eventRecord.setEventType(StockStatus.STOCK_ADDED.name());
        eventRecord.setEventId(UUID.randomUUID().getMostSignificantBits());
        eventRecord.setEntityId(event.getStockDetails().getName());
        eventRecord.setEventTime(LocalDateTime.now());

        CompletableFuture<SendResult<String, Object>> future = template.send("stock", eventRecord);
        future.whenComplete((result, ex) -> {
            if (ex == null) {
                System.out.println("Sent message=[" + eventRecord +
                        "] with offset=[" + result.getRecordMetadata().offset() + "]");
            } else {
                System.out.println("Unable to send message=[" +
                        eventRecord + "] due to : " + ex.getMessage());
            }
        });
    }

    public void addEvent(StockRemovedEvent event) throws JsonProcessingException {
        EventRecord eventRecord = new EventRecord();
        eventRecord.setEventData(new ObjectMapper().writeValueAsString(event.getStockDetails()));
        eventRecord.setEventType(StockStatus.STOCK_REMOVED.name());
        eventRecord.setEventId(UUID.randomUUID().getMostSignificantBits());
        eventRecord.setEntityId(event.getStockDetails().getName());
        eventRecord.setEventTime(LocalDateTime.now());
        CompletableFuture<SendResult<String, Object>> future = template.send("stock", eventRecord);
        future.whenComplete((result, ex) -> {
            if (ex == null) {
                System.out.println("Sent message=[" + eventRecord +
                        "] with offset=[" + result.getRecordMetadata().offset() + "]");
            } else {
                System.out.println("Unable to send message=[" +
                        eventRecord + "] due to : " + ex.getMessage());
            }
        });
    }


}

Kafka config:
package com.ibm.eventsourcing.kafka;

import org.apache.kafka.clients.admin.NewTopic;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.common.serialization.StringSerializer;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.kafka.core.DefaultKafkaProducerFactory;
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.kafka.core.ProducerFactory;
import org.springframework.kafka.support.serializer.JsonSerializer;

import java.util.HashMap;
import java.util.Map;

@Configuration
public class KafkaProducerConfig {

    @Bean
    public NewTopic createTopic() {
        return new NewTopic("stock", 3, (short) 1);
    }
    @Bean
    public Map<String, Object> producerConfig() {
        Map<String, Object> props = new HashMap<>();
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,
                "localhost:9092");
        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,
                StringSerializer.class);
        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,
                JsonSerializer.class);
        return props;
    }

    @Bean
    public ProducerFactory<String, Object> producerFactory() {
        return new DefaultKafkaProducerFactory<>(producerConfig());
    }

    @Bean
    public KafkaTemplate<String, Object> kafkaTemplate() {
        return new KafkaTemplate<>(producerFactory());
    }

}
package com.ibm.eventsourcing.kafka;

public enum StockStatus {
    STOCK_ADDED,
    STOCK_REMOVED
}
package com.ibm.eventsourcing.kafka;

import com.fasterxml.jackson.core.JsonProcessingException;
import com.google.gson.Gson;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.web.bind.annotation.*;

import java.time.LocalDate;
import java.time.LocalDateTime;
import java.util.List;

@RestController
public class StockController {

    @Autowired
    private EventService eventService;
    @Autowired
    private StockRepo repo;

    @PostMapping("/stock")
    public void addStock(@RequestBody Stock stockRequest) throws JsonProcessingException {
        StockAddedEvent event = StockAddedEvent.builder().stockDetails(stockRequest).build();

        List<Stock> existingStockList = repo.findByName(stockRequest.getName());

        if (existingStockList != null && existingStockList.size() > 0) {

            Stock existingStock = existingStockList.get(0);
            int newQuantity = existingStock.getQuantity() + stockRequest.getQuantity();
            existingStock.setQuantity(newQuantity);
            existingStock.setUserName(stockRequest.getUserName());
            repo.save(existingStock);

        } else {
            repo.save(stockRequest);
        }
        //add Event
        eventService.addEvent(event);
    }

    @DeleteMapping("/stock")
    public void removeStock(@RequestBody Stock stock) throws JsonProcessingException {
        StockRemovedEvent event = StockRemovedEvent.builder().stockDetails(stock).build();
        int newQuantity = 0;

        List<Stock> existingStockList = repo.findByName(stock.getName());

        if (existingStockList != null && existingStockList.size() > 0) {

            Stock existingStock = existingStockList.get(0);

            newQuantity = existingStock.getQuantity() - stock.getQuantity();

            if (newQuantity <= 0) {
                repo.delete(existingStock);
            } else {
                existingStock.setQuantity(newQuantity);
                existingStock.setUserName(stock.getUserName());
                repo.save(existingStock);
            }
        }
        //fire delete event
        eventService.addEvent(event);
    }

    @GetMapping("/stock")
    public List<Stock> getStock(@RequestParam("name") String name) throws JsonProcessingException {
        return repo.findByName(name);
    }
}
package com.ibm.eventsourcing.kafka;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;

@SpringBootApplication
public class EventsourcingKakfaApplication {

	public static void main(String[] args) {
		SpringApplication.run(EventsourcingKakfaApplication.class, args);
	}

}

Testing:

POST  localhost:8080/stock
{
    "name": "Iphone",
    "quantity": 20,
    "userName": "Subramanian Murugan"
}

DELETE  localhost:8080/stock
{
    "name": "Iphone",
    "quantity": 20,
    "userName": "Subramanian Murugan"
}

Watch Events in Live:

docker ps
CONTAINER ID   IMAGE                             COMMAND                  CREATED          STATUS          PORTS                                        NAMES
33cdca90a21f   confluentinc/cp-kafka:7.5.0       "/etc/confluent/dock…"   11 minutes ago   Up 11 minutes   0.0.0.0:9092->9092/tcp                       kafka
b64e0664098e   confluentinc/cp-zookeeper:7.5.0   "/etc/confluent/dock…"   11 minutes ago   Up 11 minutes   2888/tcp, 0.0.0.0:2181->2181/tcp, 3888/tcp   zookeeper

find container id of running kafka

docker exec -it 33cdca90a21f /bin/bash

[appuser@33cdca90a21f ~]$ kafka-console-consumer --bootstrap-server localhost:9092 --topic stock --from-beginning

{"eventId":-4391364360740974911,"eventType":"STOCK_ADDED","entityId":"Iphone","eventData":"{\"id\":1,\"name\":\"Iphone\",\"quantity\":20,\"userName\":\"Subramanian Murugan\"}","eventTime":[2025,8,6,12,27,36,789237600]}
{"eventId":-4746615135167364213,"eventType":"STOCK_ADDED","entityId":"Iphone","eventData":"{\"id\":0,\"name\":\"Iphone\",\"quantity\":20,\"userName\":\"Subramanian Murugan\"}","eventTime":[2025,8,6,12,28,34,357735700]}
{"eventId":-1529879407715728590,"eventType":"STOCK_REMOVED","entityId":"Iphone","eventData":"{\"id\":0,\"name\":\"Iphone\",\"quantity\":10,\"userName\":\"Subramanian Murugan\"}","eventTime":[2025,8,6,12,29,7,45768400]}
{"eventId":7016807850591079952,"eventType":"STOCK_REMOVED","entityId":"Iphone","eventData":"{\"id\":0,\"name\":\"Iphone\",\"quantity\":10,\"userName\":\"Subramanian Murugan\"}","eventTime":[2025,8,6,12,29,28,81031100]}

&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&
								Event Sourcing Design pattern using    Spring Cloud Stream
&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&
Spring Cloud Stream is a framework for building highly scalable event-driven microservices connected with shared messaging systems.

 Spring Cloud Stream is a Spring module that merges Spring Integration (which implements integration patterns) with Spring Boot.
The goal of this module is to allow the developer to focus solely on the business logic of event-driven applications, without worrying about the code to handle different types of message systems.

In fact, with Spring Cloud Stream, you can write code to produce/consume messages on Kafka, but the same code would also work if you used RabbitMQ, AWS Kinesis, AWS SQS, Azure EventHubs, etc!

The framework provides a flexible programming model built on already established and familiar Spring idioms and best practices, including support for persistent pub/sub semantics, consumer groups, and stateful partitions.

Spring Cloud Stream from Spring Cloud Function:

Spring Cloud Stream is based on Spring Cloud Function. Business logic can be written through simple functions.

The classic three interfaces of Java are used:

Supplier: a function that has output but no input; it is also called producer, publisher, source .
Consumer: a function that has input but no output, it is also called subscriber or sink.
Function: a function that has both input and output, is also called processor

					Spring Cloud Stream
					 	  |
 			    Kafka or Google Pub sub or RabbitMQ			
 


Binder Implementations:
 Binder is bridge api which connects Messaging providers.

RabbitMQ
Apache Kafka
Kafka Streams
Amazon Kinesis
Google PubSub (partner maintained)
Solace PubSub+ (partner maintained)
Azure Event Hubs (partner maintained)
Azure Service Bus (partner maintained)
AWS SQS (partner maintained)
AWS SNS (partner maintained)
Apache RocketMQ (partner maintained)

The core building blocks of Spring Cloud Stream are:

1.Destination Binders: Components responsible to provide integration with the external messaging systems.

2.Destination Bindings: Bridge between the external messaging systems and application code (producer/consumer) provided by the end user.

3.Message: The canonical data structure used by producers and consumers to communicate with Destination Binders (and thus other applications via external messaging systems).


Spring Cloud Stream application Types

1.Sources - java.util.function.Supplier
2.Sinks -java.util.function.Consumer
3.Processors -java.util.function.Function

				Modern Spring Cloud Stream bindings works with functional Style rather than annotation style.


Two types of programming.

1.Publising events automatically
2.Publishing events manually.


1.Publising events automatically

=>Publisher 
=>Consumer
=>Processor

Note: 
 The publisher,consumers,processor are represented as "Functional Bean".

By default we don't need any configurations related to connecting Kafka, providing topic name....

pom.xml
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
	xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd">
	<modelVersion>4.0.0</modelVersion>
	<parent>
		<groupId>org.springframework.boot</groupId>
		<artifactId>spring-boot-starter-parent</artifactId>
		<version>3.5.4</version>
		<relativePath/> <!-- lookup parent from repository -->
	</parent>
	<groupId>com.ibm.cloud.stream</groupId>
	<artifactId>springcloudstream</artifactId>
	<version>0.0.1-SNAPSHOT</version>
	<name>springcloudstream</name>
	<description>Demo project for Spring Boot</description>
	<url/>
	<licenses>
		<license/>
	</licenses>
	<developers>
		<developer/>
	</developers>
	<scm>
		<connection/>
		<developerConnection/>
		<tag/>
		<url/>
	</scm>
	<properties>
		<java.version>17</java.version>
		<spring-cloud.version>2025.0.0</spring-cloud.version>
	</properties>
	<dependencies>
		<dependency>
			<groupId>org.springframework.boot</groupId>
			<artifactId>spring-boot-starter-web</artifactId>
		</dependency>
		<dependency>
			<groupId>org.springframework.cloud</groupId>
			<artifactId>spring-cloud-stream</artifactId>
		</dependency>
		<dependency>
			<groupId>org.springframework.cloud</groupId>
			<artifactId>spring-cloud-stream-binder-kafka</artifactId>
		</dependency>
		<dependency>
			<groupId>org.springframework.boot</groupId>
			<artifactId>spring-boot-starter-test</artifactId>
			<scope>test</scope>
		</dependency>
		<dependency>
			<groupId>org.springframework.cloud</groupId>
			<artifactId>spring-cloud-stream-test-binder</artifactId>
			<scope>test</scope>
		</dependency>
		<dependency>
			<groupId>org.springframework.kafka</groupId>
			<artifactId>spring-kafka-test</artifactId>
			<scope>test</scope>
		</dependency>
	</dependencies>
	<dependencyManagement>
		<dependencies>
			<dependency>
				<groupId>org.springframework.cloud</groupId>
				<artifactId>spring-cloud-dependencies</artifactId>
				<version>${spring-cloud.version}</version>
				<type>pom</type>
				<scope>import</scope>
			</dependency>
		</dependencies>
	</dependencyManagement>

	<build>
		<plugins>
			<plugin>
				<groupId>org.springframework.boot</groupId>
				<artifactId>spring-boot-maven-plugin</artifactId>
			</plugin>
		</plugins>
	</build>

</project>


application.yml
spring:
  cloud:
    function:
      definition: stringSupplier;stringConsumer
    stream:
      bindings:
        stringSupplier-out-0:
          destination: randomUUid-topic
        stringConsumer-in-0:
          destination: randomUUid-topic


package com.ibm.cloud.stream;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.context.annotation.Bean;

import java.util.UUID;
import java.util.function.Consumer;
import java.util.function.Supplier;

@SpringBootApplication
public class SpringcloudstreamApplication {

    public static void main(String[] args) {
        SpringApplication.run(SpringcloudstreamApplication.class, args);
    }

    @Bean
    public Supplier<UUID> stringSupplier() {
        return () -> {
            var uuid = UUID.randomUUID();
            return uuid;
        };
    }

    @Bean
    public Consumer<String> stringConsumer() {
        return message -> {
            System.out.println(message);
        };
    }
}
.....................................................................................
		How to publish message via rest api using Spring Cloud Stream
			 (Event Sourcing)
.....................................................................................

StreamBridge - Api used to publish message using rest api.

application.yml
#Stream Configuration
spring:
  cloud:
    function:
      definition: stringSupplier;stringConsumer;stockEventConsumer
    stream:
      bindings:
        stringSupplier-out-0:
          destination: randomUUid-topic
        stringConsumer-in-0:
          destination: randomUUid-topic
        stockEvent-out-0:
          destination: inventory-topic
        stockEventConsumer-in-0:
          destination: inventory-topic
#Bindiner(Kafka) Configuration

package com.ibm;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.context.annotation.Bean;

import java.util.UUID;
import java.util.function.Consumer;
import java.util.function.Supplier;

@SpringBootApplication
public class SpringcloudstreamApplication {

    public static void main(String[] args) {
        SpringApplication.run(SpringcloudstreamApplication.class, args);
    }

    //producer which sends messages via functional
    //stringSupplier is function name , if you dont configure, then function name would be topic
    //name
    @Bean
    public Supplier<UUID> stringSupplier() {
        return () -> {
            var uuid = UUID.randomUUID();
            return uuid;
        };
    }

	//Consumer

    @Bean
    public Consumer<UUID> stringConsumer() {
        return uuid -> {
            System.out.println("Received: " + uuid);
        };
    }

    @Bean
    public Consumer<Stock> stockEventConsumer() {
        return stock -> {
            System.out.println("Received: " + stock);
        };
    }
}
package com.ibm;

import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.cloud.stream.function.StreamBridge;
import org.springframework.web.bind.annotation.PostMapping;
import org.springframework.web.bind.annotation.RequestBody;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.RestController;

@RestController
@RequestMapping("/api/publish")
public class StockController {

    @Autowired
    private StreamBridge streamBridge;

    @PostMapping
    public String publish(@RequestBody Stock stock) {
        streamBridge.send("stockEvent-out-0", stock);
        return "Message Published";

    }
}


